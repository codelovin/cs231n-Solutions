{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch Normalization\n",
    "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization which was recently proposed by [3].\n",
    "\n",
    "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
    "\n",
    "The authors of [3] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [3] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n",
    "\n",
    "It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n",
    "\n",
    "[3] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Batch normalization: Forward\n",
    "In the file `cs231n/layers.py`, implement the batch normalization forward pass in the function `batchnorm_forward`. Once you have done so, run the following to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:  [ 27.18502186  34.21455511  37.68611762]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [  4.44089210e-17   8.27116153e-17   4.46864767e-17]\n",
      "  std:  [ 0.99999999  1.          1.        ]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 0.99999999  1.99999999  2.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=0))\n",
    "print('  stds: ', a.std(axis=0))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print('  mean: ', a_norm.mean(axis=0))\n",
    "print('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print('After batch normalization (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.03927354 -0.04349152 -0.10452688]\n",
      "  stds:  [ 1.01531428  1.01238373  0.97819988]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Batch Normalization: backward\n",
    "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
    "\n",
    "To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass.\n",
    "\n",
    "Once you have finished, run the following to numerically check your backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.66746048753e-09\n",
      "dgamma error:  7.41722504069e-13\n",
      "dbeta error:  2.37944694996e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Batch Normalization: alternative backward (OPTIONAL, +3 points extra credit)\n",
    "In class we talked about two different implementations for the sigmoid backward pass. One strategy is to write out a computation graph composed of simple operations and backprop through all intermediate values. Another strategy is to work out the derivatives on paper. For the sigmoid function, it turns out that you can derive a very simple formula for the backward pass by simplifying gradients on paper.\n",
    "\n",
    "Surprisingly, it turns out that you can also derive a simple expression for the batch normalization backward pass if you work out derivatives on paper and simplify. After doing so, implement the simplified batch normalization backward pass in the function `batchnorm_backward_alt` and compare the two implementations by running the following. Your two implementations should compute nearly identical results, but the alternative implementation should be a bit faster.\n",
    "\n",
    "NOTE: This part of the assignment is entirely optional, but we will reward 3 points of extra credit if you can complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-57f7cde20045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dx difference: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dgamma difference: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgamma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgamma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dbeta difference: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-249f81ca5e9d>\u001b[0m in \u001b[0;36mrel_error\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;34m\"\"\" returns relative error \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx difference: ', rel_error(dx1, dx2))\n",
    "print('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fully Connected Nets with Batch Normalization\n",
    "Now that you have a working implementation for batch normalization, go back to your `FullyConnectedNet` in the file `cs2312n/classifiers/fc_net.py`. Modify your implementation to add batch normalization.\n",
    "\n",
    "Concretely, when the flag `use_batchnorm` is `True` in the constructor, you should insert a batch normalization layer before each ReLU nonlinearity. The outputs from the last layer of the network should not be normalized. Once you are done, run the following to gradient-check your implementation.\n",
    "\n",
    "HINT: You might find it useful to define an additional helper layer similar to those in the file `cs231n/layer_utils.py`. If you decide to do so, do it in the file `cs231n/classifiers/fc_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "{'W3': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.05373403,\n",
      "         0.05718522,  0.        ,  0.04897696,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05374087,\n",
      "         0.05719249,  0.        ,  0.04898318,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05386844,\n",
      "         0.05732826,  0.        ,  0.04909946,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05399552,\n",
      "         0.0574635 ,  0.        ,  0.04921529,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05400206,\n",
      "         0.05747046,  0.        ,  0.04922125,  0.        ,  0.        ],\n",
      "       [ 0.05947625,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911472,  0.04696487, -0.44659814,  0.        ,  0.05888977],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05399999,\n",
      "         0.05746825,  0.        ,  0.04921936,  0.        ,  0.        ],\n",
      "       [ 0.05925841,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04893482,  0.04679286, -0.44496244,  0.        ,  0.05867408],\n",
      "       [ 0.05940538,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04905618,  0.0469089 , -0.44606596,  0.        ,  0.05881959],\n",
      "       [ 0.05416711,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.0447305 ,  0.04277256, -0.40673262,  0.        ,  0.05363298],\n",
      "       [ 0.05948247,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911985,  0.04696979, -0.44664487,  0.        ,  0.05889593],\n",
      "       [ 0.05947322,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911221,  0.04696248, -0.44657543,  0.        ,  0.05888677],\n",
      "       [ 0.05947652,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911494,  0.04696508, -0.44660014,  0.        ,  0.05889003],\n",
      "       [ 0.05948238,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911978,  0.04696971, -0.44664419,  0.        ,  0.05889584],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.04982457,\n",
      "         0.05302466,  0.        ,  0.04541359,  0.        ,  0.        ],\n",
      "       [ 0.05940232,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04905367,  0.04690649, -0.44604301,  0.        ,  0.05881657],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.0539872 ,\n",
      "         0.05745464,  0.        ,  0.04920771,  0.        ,  0.        ],\n",
      "       [ 0.05890176,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04864031,  0.04651123, -0.44228438,  0.        ,  0.05832094],\n",
      "       [ 0.05939585,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04904832,  0.04690138, -0.44599444,  0.        ,  0.05881016],\n",
      "       [ 0.05947473,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04911346,  0.04696367, -0.44658673,  0.        ,  0.05888826],\n",
      "       [ 0.05944843,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04909174,  0.0469429 , -0.44638926,  0.        ,  0.05886222],\n",
      "       [ 0.05946748,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04910747,  0.04695794, -0.44653225,  0.        ,  0.05888108],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05396357,\n",
      "         0.0574295 ,  0.        ,  0.04918617,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05229606,\n",
      "         0.05565489,  0.        ,  0.04766629,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05396161,\n",
      "         0.05742741,  0.        ,  0.04918438,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05400492,\n",
      "         0.05747351,  0.        ,  0.04922386,  0.        ,  0.        ],\n",
      "       [ 0.05946429,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04910484,  0.04695543, -0.44650835,  0.        ,  0.05887793],\n",
      "       [ 0.05946166,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04910266,  0.04695335, -0.44648859,  0.        ,  0.05887533],\n",
      "       [ 0.05935444,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.04901413,  0.04686869, -0.44568351,  0.        ,  0.05876916],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05399628,\n",
      "         0.05746431,  0.        ,  0.04921599,  0.        ,  0.        ]], dtype=float32), 'b3': array([ 0.0594891 ,  0.        ,  0.        ,  0.        ,  0.05402144,\n",
      "        0.10661642,  0.04697502, -0.39745575,  0.        ,  0.0589025 ], dtype=float32), 'W2': array([[  6.48573914e-04,  -3.56789940e-04,  -1.07306660e-04,\n",
      "          5.50152754e-06,   5.01362774e-06,   1.87896239e-05,\n",
      "         -1.44710575e-05,  -6.21414336e-04,  -1.23896301e-04,\n",
      "          1.39380887e-01,   7.01323415e-06,  -1.31665129e-05,\n",
      "         -1.27427961e-06,  -4.67131485e-06,  -1.23405540e-02,\n",
      "          1.62957513e-04,  -3.31902356e-06,  -4.50409949e-03,\n",
      "          5.52135680e-06,  -1.35422215e-05,  -2.76605308e-04,\n",
      "          7.43903802e-05,  -1.92206462e-05,  -5.58256777e-03,\n",
      "          2.83503741e-05,  -1.40181201e-05,  -4.55526279e-05,\n",
      "          4.54190413e-05,  -1.54668945e-04,   5.74991282e-06],\n",
      "       [ -1.15042971e-03,   6.32868148e-04,   1.90338789e-04,\n",
      "         -9.75851890e-06,  -8.89309013e-06,  -3.33287280e-05,\n",
      "          2.56685253e-05,   1.10225449e-03,   2.19765221e-04,\n",
      "         -2.47231528e-01,  -1.24362577e-05,   2.33545434e-05,\n",
      "          2.26029624e-06,   8.28590237e-06,   2.18894724e-02,\n",
      "         -2.89051357e-04,   5.88722969e-06,   7.98929762e-03,\n",
      "         -9.79369179e-06,   2.40209702e-05,   4.90637962e-04,\n",
      "         -1.31952431e-04,   3.40932675e-05,   9.90226772e-03,\n",
      "         -5.02874282e-05,   2.48651104e-05,   8.08005061e-05,\n",
      "         -8.05635500e-05,   2.74349237e-04,  -1.01991009e-05],\n",
      "       [  1.14507123e-03,  -6.29920338e-04,  -1.89452228e-04,\n",
      "          9.71306599e-06,   8.85166810e-06,   3.31734846e-05,\n",
      "         -2.55489649e-05,  -1.09712034e-03,  -2.18741596e-04,\n",
      "          2.46079966e-01,   1.23820164e-05,  -2.32457623e-05,\n",
      "         -2.24976816e-06,  -8.24730705e-06,  -2.17875149e-02,\n",
      "          2.87705014e-04,  -5.85980797e-06,  -7.95208476e-03,\n",
      "          9.74807426e-06,  -2.39090841e-05,  -4.88352613e-04,\n",
      "          1.31337816e-04,  -3.39344624e-05,  -9.85614397e-03,\n",
      "          5.00531969e-05,  -2.47492935e-05,  -8.04241427e-05,\n",
      "          8.01882998e-05,  -2.73071375e-04,   1.01515952e-05],\n",
      "       [  1.15716015e-03,  -6.36570679e-04,  -1.91452345e-04,\n",
      "          9.81561061e-06,   8.94511868e-06,   3.35237128e-05,\n",
      "         -2.58186956e-05,  -1.10870309e-03,  -2.21050926e-04,\n",
      "          2.48677924e-01,   1.25127381e-05,  -2.34911768e-05,\n",
      "         -2.27351984e-06,  -8.33437753e-06,  -2.20175330e-02,\n",
      "          2.90742406e-04,  -5.92167225e-06,  -8.03603791e-03,\n",
      "          9.85098814e-06,  -2.41615016e-05,  -4.93508356e-04,\n",
      "          1.32724410e-04,  -3.42927233e-05,  -9.96019971e-03,\n",
      "          5.05816279e-05,  -2.50105804e-05,  -8.12732178e-05,\n",
      "          8.10348793e-05,  -2.75954284e-04,   1.02587701e-05],\n",
      "       [ -1.15747028e-03,   6.36741228e-04,   1.91503641e-04,\n",
      "         -9.81823996e-06,  -8.94751520e-06,  -3.35326949e-05,\n",
      "          2.58256132e-05,   1.10900018e-03,   2.21110167e-04,\n",
      "         -2.48744562e-01,  -1.25123661e-05,   2.34974723e-05,\n",
      "          2.27412920e-06,   8.33661124e-06,   2.20234338e-02,\n",
      "         -2.90820317e-04,   5.92325887e-06,   8.03819112e-03,\n",
      "         -9.85362840e-06,   2.41679754e-05,   4.93640604e-04,\n",
      "         -1.32759975e-04,   3.43019128e-05,   9.96286795e-03,\n",
      "         -5.05951830e-05,   2.50172834e-05,   8.12949947e-05,\n",
      "         -8.10565907e-05,   2.76028237e-04,  -1.02615186e-05],\n",
      "       [ -1.15690415e-03,   6.36429817e-04,   1.91409985e-04,\n",
      "         -9.81343874e-06,  -8.94313871e-06,  -3.35162949e-05,\n",
      "          2.58129821e-05,   1.10845780e-03,   2.21002017e-04,\n",
      "         -2.48622894e-01,  -1.25062470e-05,   2.34859799e-05,\n",
      "          2.27301689e-06,   8.33253307e-06,   2.20126621e-02,\n",
      "         -2.90678086e-04,   5.92036167e-06,   8.03426001e-03,\n",
      "         -9.84880899e-06,   2.41561556e-05,   4.93399159e-04,\n",
      "         -1.32695030e-04,   3.42851345e-05,   9.95799527e-03,\n",
      "         -5.05704375e-05,   2.50050471e-05,   8.12552316e-05,\n",
      "         -8.10169440e-05,   2.75893224e-04,  -1.02565000e-05],\n",
      "       [  1.15703768e-03,  -6.36503275e-04,  -1.91432089e-04,\n",
      "          9.81457106e-06,   8.94417190e-06,   3.35201621e-05,\n",
      "         -2.58159635e-05,  -1.10858574e-03,  -2.21027542e-04,\n",
      "          2.48651609e-01,   1.25114138e-05,  -2.34886902e-05,\n",
      "         -2.27327928e-06,  -8.33349532e-06,  -2.20152028e-02,\n",
      "          2.90711643e-04,  -5.92104561e-06,  -8.03518761e-03,\n",
      "          9.84994585e-06,  -2.41589441e-05,  -4.93456144e-04,\n",
      "          1.32710353e-04,  -3.42890962e-05,  -9.95914545e-03,\n",
      "          5.05762764e-05,  -2.50079338e-05,  -8.12646176e-05,\n",
      "          8.10263009e-05,  -2.75925093e-04,   1.02576842e-05],\n",
      "       [  1.15578843e-03,  -6.35816075e-04,  -1.91225394e-04,\n",
      "          9.80397454e-06,   8.93451488e-06,   3.34839715e-05,\n",
      "         -2.57880893e-05,  -1.10738887e-03,  -2.20788905e-04,\n",
      "          2.48383150e-01,   1.24979060e-05,  -2.34633299e-05,\n",
      "         -2.27082478e-06,  -8.32449768e-06,  -2.19914336e-02,\n",
      "          2.90397758e-04,  -5.91465232e-06,  -8.02651234e-03,\n",
      "          9.83931113e-06,  -2.41328617e-05,  -4.92923369e-04,\n",
      "          1.32567075e-04,  -3.42520725e-05,  -9.94839240e-03,\n",
      "          5.05216703e-05,  -2.49809327e-05,  -8.11768768e-05,\n",
      "          8.09388221e-05,  -2.75627186e-04,   1.02466092e-05],\n",
      "       [ -1.15666108e-03,   6.36296114e-04,   1.91369763e-04,\n",
      "         -9.81137691e-06,  -8.94125969e-06,  -3.35092518e-05,\n",
      "          2.58075597e-05,   1.10822485e-03,   2.20955582e-04,\n",
      "         -2.48570666e-01,  -1.25036186e-05,   2.34810450e-05,\n",
      "          2.27253918e-06,   8.33078320e-06,   2.20080372e-02,\n",
      "         -2.90616998e-04,   5.91911794e-06,   8.03257246e-03,\n",
      "         -9.84673989e-06,   2.41510807e-05,   4.93295491e-04,\n",
      "         -1.32667163e-04,   3.42779313e-05,   9.95590352e-03,\n",
      "         -5.05598109e-05,   2.49997938e-05,   8.12381622e-05,\n",
      "         -8.09999256e-05,   2.75835278e-04,  -1.02543445e-05],\n",
      "       [  1.14805554e-03,  -6.31562085e-04,  -1.89945975e-04,\n",
      "          9.73837996e-06,   8.87473743e-06,   3.32599448e-05,\n",
      "         -2.56155508e-05,  -1.09997974e-03,  -2.19311682e-04,\n",
      "          2.46721312e-01,   1.24142871e-05,  -2.33063456e-05,\n",
      "         -2.25563167e-06,  -8.26880205e-06,  -2.18442976e-02,\n",
      "          2.88454816e-04,  -5.87507975e-06,  -7.97280949e-03,\n",
      "          9.77348009e-06,  -2.39713972e-05,  -4.89625381e-04,\n",
      "          1.31680121e-04,  -3.40229053e-05,  -9.88183171e-03,\n",
      "          5.01836475e-05,  -2.48137949e-05,  -8.06337484e-05,\n",
      "          8.03972871e-05,  -2.73783051e-04,   1.01780524e-05],\n",
      "       [  1.15627225e-03,  -6.36082201e-04,  -1.91305429e-04,\n",
      "          9.80807818e-06,   8.93825381e-06,   3.34979850e-05,\n",
      "         -2.57988831e-05,  -1.10785232e-03,  -2.20881309e-04,\n",
      "          2.48487100e-01,   1.25031365e-05,  -2.34731506e-05,\n",
      "         -2.27177520e-06,  -8.32798196e-06,  -2.20006388e-02,\n",
      "          2.90519296e-04,  -5.91712796e-06,  -8.02987162e-03,\n",
      "          9.84342932e-06,  -2.41429607e-05,  -4.93129657e-04,\n",
      "          1.32622561e-04,  -3.42664098e-05,  -9.95255634e-03,\n",
      "          5.05428143e-05,  -2.49913883e-05,  -8.12108483e-05,\n",
      "          8.09726916e-05,  -2.75742525e-04,   1.02508975e-05],\n",
      "       [ -1.15615909e-03,   6.36019977e-04,   1.91286716e-04,\n",
      "         -9.80711866e-06,  -8.93737979e-06,  -3.34947108e-05,\n",
      "          2.57963602e-05,   1.10774394e-03,   2.20859700e-04,\n",
      "         -2.48462796e-01,  -1.24981925e-05,   2.34708550e-05,\n",
      "          2.27155306e-06,   8.32716796e-06,   2.19984874e-02,\n",
      "         -2.90490891e-04,   5.91654907e-06,   8.02908652e-03,\n",
      "         -9.84246617e-06,   2.41405996e-05,   4.93081403e-04,\n",
      "         -1.32609581e-04,   3.42630556e-05,   9.95158311e-03,\n",
      "         -5.05378703e-05,   2.49889436e-05,   8.12029029e-05,\n",
      "         -8.09647754e-05,   2.75715574e-04,  -1.02498952e-05],\n",
      "       [ -1.15719135e-03,   6.36587793e-04,   1.91457497e-04,\n",
      "         -9.81587436e-06,  -8.94535879e-06,  -3.35246150e-05,\n",
      "          2.58193886e-05,   1.10873289e-03,   2.21056878e-04,\n",
      "         -2.48684615e-01,  -1.25093511e-05,   2.34918080e-05,\n",
      "          2.27358100e-06,   8.33460126e-06,   2.20181253e-02,\n",
      "         -2.90750235e-04,   5.92183142e-06,   8.03625397e-03,\n",
      "         -9.85125371e-06,   2.41621510e-05,   4.93521628e-04,\n",
      "         -1.32727975e-04,   3.42936473e-05,   9.96046700e-03,\n",
      "         -5.05829885e-05,   2.50112535e-05,   8.12754006e-05,\n",
      "         -8.10370548e-05,   2.75961705e-04,  -1.02590457e-05],\n",
      "       [ -1.15717342e-03,   6.36577955e-04,   1.91454543e-04,\n",
      "         -9.81572248e-06,  -8.94522054e-06,  -3.35240948e-05,\n",
      "          2.58189903e-05,   1.10871578e-03,   2.21053459e-04,\n",
      "         -2.48680770e-01,  -1.25091574e-05,   2.34914460e-05,\n",
      "          2.27354599e-06,   8.33447302e-06,   2.20177863e-02,\n",
      "         -2.90745753e-04,   5.92174001e-06,   8.03613011e-03,\n",
      "         -9.85110091e-06,   2.41617781e-05,   4.93514002e-04,\n",
      "         -1.32725923e-04,   3.42931162e-05,   9.96031333e-03,\n",
      "         -5.05822063e-05,   2.50108678e-05,   8.12741491e-05,\n",
      "         -8.10358033e-05,   2.75957456e-04,  -1.02588874e-05],\n",
      "       [ -1.15719822e-03,   6.36591576e-04,   1.91458632e-04,\n",
      "         -9.81593257e-06,  -8.94541245e-06,  -3.35248114e-05,\n",
      "          2.58195432e-05,   1.10873953e-03,   2.21058188e-04,\n",
      "         -2.48686090e-01,  -1.25094257e-05,   2.34919480e-05,\n",
      "          2.27359465e-06,   8.33465128e-06,   2.20182575e-02,\n",
      "         -2.90751981e-04,   5.92186643e-06,   8.03630240e-03,\n",
      "         -9.85131192e-06,   2.41622965e-05,   4.93524596e-04,\n",
      "         -1.32728761e-04,   3.42938511e-05,   9.96052660e-03,\n",
      "         -5.05832904e-05,   2.50114026e-05,   8.12758881e-05,\n",
      "         -8.10375423e-05,   2.75963364e-04,  -1.02591066e-05],\n",
      "       [ -1.15609611e-03,   6.35985343e-04,   1.91276296e-04,\n",
      "         -9.80658479e-06,  -8.93689321e-06,  -3.34928845e-05,\n",
      "          2.57949541e-05,   1.10768364e-03,   2.20847665e-04,\n",
      "         -2.48449251e-01,  -1.24975122e-05,   2.34695763e-05,\n",
      "          2.27142937e-06,   8.32671412e-06,   2.19972879e-02,\n",
      "         -2.90475058e-04,   5.91622711e-06,   8.02864879e-03,\n",
      "         -9.84193048e-06,   2.41392845e-05,   4.93054569e-04,\n",
      "         -1.32602363e-04,   3.42611893e-05,   9.95104108e-03,\n",
      "         -5.05351163e-05,   2.49875829e-05,   8.11984792e-05,\n",
      "         -8.09603662e-05,   2.75700557e-04,  -1.02493368e-05],\n",
      "       [ -1.15587655e-03,   6.35864562e-04,   1.91239975e-04,\n",
      "         -9.80472214e-06,  -8.93519609e-06,  -3.34865254e-05,\n",
      "          2.57900556e-05,   1.10747328e-03,   2.20805727e-04,\n",
      "         -2.48402074e-01,  -1.24951384e-05,   2.34651197e-05,\n",
      "          2.27099804e-06,   8.32513251e-06,   2.19931100e-02,\n",
      "         -2.90419906e-04,   5.91510343e-06,   8.02712422e-03,\n",
      "         -9.84006147e-06,   2.41347007e-05,   4.92960913e-04,\n",
      "         -1.32577174e-04,   3.42546846e-05,   9.94915050e-03,\n",
      "         -5.05255193e-05,   2.49828372e-05,   8.11830614e-05,\n",
      "         -8.09449921e-05,   2.75648199e-04,  -1.02473905e-05],\n",
      "       [ -5.32697421e-04,   2.93044635e-04,   8.81348824e-05,\n",
      "         -4.51860569e-06,  -4.11787551e-06,  -1.54326044e-05,\n",
      "          1.18856087e-05,   5.10390266e-04,   1.01760561e-04,\n",
      "         -1.14478618e-01,  -5.75851118e-06,   1.08141376e-05,\n",
      "          1.04661240e-06,   3.83672159e-06,   1.01357484e-02,\n",
      "         -1.33842957e-04,   2.72603529e-06,   3.69938160e-03,\n",
      "         -4.53489201e-06,   1.11227218e-05,   2.27186043e-04,\n",
      "         -6.10995339e-05,   1.57866180e-05,   4.58516693e-03,\n",
      "         -2.32851980e-05,   1.15135936e-05,   3.74140363e-05,\n",
      "         -3.73043185e-05,   1.27035266e-04,  -4.72261354e-06],\n",
      "       [  1.15628145e-03,  -6.36087265e-04,  -1.91306957e-04,\n",
      "          9.80815639e-06,   8.93832566e-06,   3.34982542e-05,\n",
      "         -2.57990887e-05,  -1.10786117e-03,  -2.20883070e-04,\n",
      "          2.48489082e-01,   1.25032366e-05,  -2.34733379e-05,\n",
      "         -2.27179339e-06,  -8.32804835e-06,  -2.20008139e-02,\n",
      "          2.90521624e-04,  -5.91717526e-06,  -8.02993588e-03,\n",
      "          9.84350754e-06,  -2.41431535e-05,  -4.93133615e-04,\n",
      "          1.32623609e-04,  -3.42666826e-05,  -9.95263550e-03,\n",
      "          5.05432181e-05,  -2.49915884e-05,  -8.12114959e-05,\n",
      "          8.09733392e-05,  -2.75744736e-04,   1.02509794e-05],\n",
      "       [  1.15619507e-03,  -6.36039767e-04,  -1.91292667e-04,\n",
      "          9.80742334e-06,   8.93765809e-06,   3.34957513e-05,\n",
      "         -2.57971624e-05,  -1.10777840e-03,  -2.20866568e-04,\n",
      "          2.48470515e-01,   1.25023025e-05,  -2.34715844e-05,\n",
      "         -2.27162377e-06,  -8.32742626e-06,  -2.19991710e-02,\n",
      "          2.90499942e-04,  -5.91673324e-06,  -8.02933611e-03,\n",
      "          9.84277267e-06,  -2.41413509e-05,  -4.93096770e-04,\n",
      "          1.32613714e-04,  -3.42641215e-05,  -9.95189231e-03,\n",
      "          5.05394419e-05,  -2.49897221e-05,  -8.12054277e-05,\n",
      "          8.09672929e-05,  -2.75724131e-04,   1.02502136e-05]], dtype=float32), 'b2': array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   3.72529030e-09,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32), 'gamma2': array([-0.00666128,  0.00379912,  0.00283935, -0.00208846, -0.00294523,\n",
      "        0.0236075 ,  0.00729684, -0.01025989, -0.00936083,  0.02027325,\n",
      "        0.02374643, -0.01204455, -0.00165166, -0.01553075,  0.00222612,\n",
      "        0.01166794,  0.00082985, -0.01827718,  0.00035488, -0.01438118,\n",
      "       -0.06173155,  0.04281432,  0.00218717,  0.0038709 , -0.0030689 ,\n",
      "        0.01046308, -0.02133453,  0.01828358, -0.00572764, -0.0022832 ], dtype=float32), 'beta2': array([-0.00669691,  0.00381896,  0.00284742, -0.00208947, -0.00294628,\n",
      "        0.0236126 ,  0.00729974, -0.01029983, -0.00937403,  0.02226512,\n",
      "        0.02374908, -0.01204776, -0.00165201, -0.01553251,  0.00241363,\n",
      "        0.01168499,  0.00083038, -0.01845943,  0.00035544, -0.01438466,\n",
      "       -0.06177378,  0.04282989,  0.00218951,  0.00399861, -0.00307231,\n",
      "        0.01046628, -0.02134344,  0.01829202, -0.00574063, -0.00228426], dtype=float32), 'W1': array([[  1.40811801e-01,   1.13609480e-04,  -1.05459083e-04,\n",
      "          7.23808535e-06,   3.78639982e-07,   6.83777944e-06,\n",
      "         -1.06885764e-05,   4.69473780e-05,  -1.73975593e-06,\n",
      "          1.32545829e-04,   9.21416722e-06,  -1.58440434e-05,\n",
      "         -2.66177562e-06,   2.66578422e-06,  -1.57220052e-06,\n",
      "         -5.55716178e-06,   2.52664468e-05,  -4.46888655e-02,\n",
      "         -3.22328269e-05,   1.38064024e-06],\n",
      "       [ -2.03902507e+00,  -1.64512172e-03,   1.52709987e-03,\n",
      "         -1.04811072e-04,  -5.48289427e-06,  -9.90144617e-05,\n",
      "          1.54775931e-04,  -6.79821358e-04,   2.51925339e-05,\n",
      "         -1.91932963e-03,  -1.33425739e-04,   2.29429628e-04,\n",
      "          3.85160856e-05,  -3.86018801e-05,   2.27662440e-05,\n",
      "          8.04704687e-05,  -3.65815242e-04,   6.47117078e-01,\n",
      "          4.66858328e-04,  -1.99888891e-05],\n",
      "       [  3.53046918e+00,   2.84844497e-03,  -2.64409627e-03,\n",
      "          1.81475072e-04,   9.49335481e-06,   1.71438529e-04,\n",
      "         -2.67986703e-04,   1.17707637e-03,  -4.36195987e-05,\n",
      "          3.32322228e-03,   2.31019920e-04,  -3.97245836e-04,\n",
      "         -6.66910128e-05,   6.68372086e-05,  -3.94186027e-05,\n",
      "         -1.39330557e-04,   6.33395277e-04,  -1.12045050e+00,\n",
      "         -8.08332232e-04,   3.46100460e-05],\n",
      "       [  1.09957731e+00,   8.87158501e-04,  -8.23513314e-04,\n",
      "          5.65210612e-05,   2.95673954e-06,   5.33951461e-05,\n",
      "         -8.34654275e-05,   3.66604654e-04,  -1.35854807e-05,\n",
      "          1.03502965e-03,   7.19519885e-05,  -1.23723643e-04,\n",
      "         -2.07705907e-05,   2.08166894e-05,  -1.22770653e-05,\n",
      "         -4.33950045e-05,   1.97272107e-04,  -3.48968357e-01,\n",
      "         -2.51760241e-04,   1.07793530e-05],\n",
      "       [  1.40643859e+00,   1.13473961e-03,  -1.05333282e-03,\n",
      "          7.22945115e-05,   3.78188292e-06,   6.82962418e-05,\n",
      "         -1.06758285e-04,   4.68913786e-04,  -1.73768094e-05,\n",
      "          1.32387737e-03,   9.20317761e-05,  -1.58251452e-04,\n",
      "         -2.65701783e-05,   2.66260449e-05,  -1.57032518e-05,\n",
      "         -5.55053375e-05,   2.52331432e-04,  -4.46355671e-01,\n",
      "         -3.22007108e-04,   1.37879560e-05],\n",
      "       [  1.06908059e+00,   8.62553134e-04,  -8.00673210e-04,\n",
      "          5.49534561e-05,   2.87473449e-06,   5.19142341e-05,\n",
      "         -8.11505088e-05,   3.56436911e-04,  -1.32086880e-05,\n",
      "          1.00632303e-03,   6.99563971e-05,  -1.20292170e-04,\n",
      "         -2.01941984e-05,   2.02393385e-05,  -1.19365623e-05,\n",
      "         -4.21914447e-05,   1.91800136e-04,  -3.39289725e-01,\n",
      "         -2.44778959e-04,   1.04803485e-05],\n",
      "       [  5.43496311e-01,   4.38502466e-04,  -4.07044106e-04,\n",
      "          2.79370925e-05,   1.46144987e-06,   2.63920210e-05,\n",
      "         -4.12550799e-05,   1.81204450e-04,  -6.71499811e-06,\n",
      "          5.11591847e-04,   3.55642478e-05,  -6.11538126e-05,\n",
      "         -1.02671820e-05,   1.02892209e-05,  -6.06827734e-06,\n",
      "         -2.14491756e-05,   9.75086587e-05,  -1.72487214e-01,\n",
      "         -1.24436454e-04,   5.32808508e-06],\n",
      "       [ -2.26774502e+00,  -1.82965701e-03,   1.69839652e-03,\n",
      "         -1.16567862e-04,  -6.09791732e-06,  -1.10121029e-04,\n",
      "          1.72137341e-04,  -7.56077759e-04,   2.80184104e-05,\n",
      "         -2.13462324e-03,  -1.48392253e-04,   2.55165040e-04,\n",
      "          4.28353378e-05,  -4.29319007e-05,   2.53199614e-05,\n",
      "          8.94969417e-05,  -4.06846870e-04,   7.19704986e-01,\n",
      "          5.19231020e-04,  -2.22309245e-05],\n",
      "       [  3.07007861e+00,   2.47699395e-03,  -2.29929318e-03,\n",
      "          1.57809845e-04,   8.25537518e-06,   1.49082116e-04,\n",
      "         -2.33039915e-04,   1.02357985e-03,  -3.79313897e-05,\n",
      "          2.88985809e-03,   2.00893803e-04,  -3.45443026e-04,\n",
      "         -5.79946573e-05,   5.81213062e-05,  -3.42782259e-05,\n",
      "         -1.21161174e-04,   5.50798432e-04,  -9.74338293e-01,\n",
      "         -7.02919904e-04,   3.00967877e-05],\n",
      "       [  8.02592576e-01,   6.47545909e-04,  -6.01090665e-04,\n",
      "          4.12552981e-05,   2.15815408e-06,   3.89736597e-05,\n",
      "         -6.09222552e-05,   2.67588446e-04,  -9.91617981e-06,\n",
      "          7.55478570e-04,   5.25184805e-05,  -9.03071341e-05,\n",
      "         -1.51611885e-05,   1.51943113e-05,  -8.96115489e-06,\n",
      "         -3.16744517e-05,   1.43991958e-04,  -2.54715532e-01,\n",
      "         -1.83760247e-04,   7.86802411e-06],\n",
      "       [ -6.00388706e-01,  -4.84404241e-04,   4.49652842e-04,\n",
      "         -3.08615054e-05,  -1.61443222e-06,  -2.91546967e-05,\n",
      "          4.55736008e-05,  -2.00172639e-04,   7.41791337e-06,\n",
      "         -5.65144466e-04,  -3.92870606e-05,   6.75553019e-05,\n",
      "          1.13438264e-05,  -1.13662809e-05,   6.70349573e-06,\n",
      "          2.36944397e-05,  -1.07719512e-04,   1.90542921e-01,\n",
      "          1.37454699e-04,  -5.88605690e-06],\n",
      "       [ -2.30792344e-01,  -1.86207340e-04,   1.72848755e-04,\n",
      "         -1.18633125e-05,  -6.20595529e-07,  -1.12072066e-05,\n",
      "          1.75187142e-05,  -7.69473336e-05,   2.85148190e-06,\n",
      "         -2.17244306e-04,  -1.51021368e-05,   2.59685876e-05,\n",
      "          4.35903348e-06,  -4.36925347e-06,   2.57685610e-06,\n",
      "          9.10825838e-06,  -4.14047172e-05,   7.32456148e-02,\n",
      "          5.28446035e-05,  -2.26243037e-06],\n",
      "       [  2.38221765e-01,   1.92201522e-04,  -1.78412942e-04,\n",
      "          1.22452038e-05,   6.40573262e-07,   1.15679795e-05,\n",
      "         -1.80826610e-05,   7.94243533e-05,  -2.94327492e-06,\n",
      "          2.24237621e-04,   1.55882881e-05,  -2.68045405e-05,\n",
      "         -4.49737126e-06,   4.50990410e-06,  -2.65980725e-06,\n",
      "         -9.40146492e-06,   4.27336054e-05,  -7.56034553e-02,\n",
      "         -5.45536750e-05,   2.33501214e-06],\n",
      "       [ -9.01294768e-01,  -7.27180624e-04,   6.75012241e-04,\n",
      "         -4.63288379e-05,  -2.42356191e-06,  -4.37666058e-05,\n",
      "          6.84144179e-05,  -3.00496235e-04,   1.11356640e-05,\n",
      "         -8.48386670e-04,  -5.89771589e-05,   1.01413032e-04,\n",
      "          1.70237254e-05,  -1.70628955e-05,   1.00631896e-05,\n",
      "          3.55697484e-05,  -1.61696022e-04,   2.86040246e-01,\n",
      "          2.06366822e-04,  -8.83538087e-06],\n",
      "       [ -1.31384563e+00,  -1.06003392e-03,   9.83986654e-04,\n",
      "         -6.75350020e-05,  -3.53290216e-06,  -6.37999474e-05,\n",
      "          9.97298412e-05,  -4.38042800e-04,   1.62328070e-05,\n",
      "         -1.23671978e-03,  -8.59728607e-05,   1.47832965e-04,\n",
      "          2.48162360e-05,  -2.48731158e-05,   1.46694265e-05,\n",
      "          5.18511370e-05,  -2.35709798e-04,   4.16969806e-01,\n",
      "          3.00826505e-04,  -1.28796373e-05]], dtype=float32), 'b1': array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "        -3.72529030e-09,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   7.45058060e-09,   0.00000000e+00,\n",
      "         1.49011612e-08,   4.65661287e-10], dtype=float32), 'gamma1': array([-0.00822882,  0.00492481,  0.00196849, -0.02435233,  0.0133843 ,\n",
      "        0.01081988,  0.0239151 , -0.01653914, -0.00170714, -0.00373006,\n",
      "       -0.00525101, -0.00795207, -0.0101273 ,  0.00944609, -0.00616611,\n",
      "       -0.00260978,  0.00961021, -0.00174179,  0.01857569, -0.00072045], dtype=float32), 'beta1': array([-0.01468651,  0.0049553 ,  0.00198995, -0.02436058,  0.01338525,\n",
      "        0.01082594,  0.02392573, -0.01656437, -0.00170845, -0.00376092,\n",
      "       -0.00525682, -0.00796165, -0.01013045,  0.00944918, -0.006168  ,\n",
      "       -0.00261307,  0.00962414, -0.0037849 ,  0.0185961 , -0.00072129], dtype=float32)}\n",
      "Initial loss:  2.28617858887\n",
      "W1 relative error: 1.00e+00\n",
      "W2 relative error: 1.00e+00\n",
      "W3 relative error: 1.00e+00\n",
      "b1 relative error: 1.00e+00\n",
      "b2 relative error: 3.73e-01\n",
      "b3 relative error: 1.00e+00\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "beta1 relative error: 1.00e+00\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "beta2 relative error: 1.00e+00\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.]\n",
      "gamma1 relative error: 1.00e+00\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "gamma2 relative error: 1.00e+00\n",
      "\n",
      "Running check with reg =  3.14\n",
      "{'W3': array([[ 0.02065716,  0.06667109,  0.28878528, -0.0045069 , -0.02896826,\n",
      "        -0.27898893,  0.02151608, -0.26617047, -0.1572607 , -0.06714477],\n",
      "       [ 0.02296575,  0.29959872,  0.18414672, -0.02681814,  0.05360002,\n",
      "        -0.03462284,  0.03931769,  0.02197256,  0.07951108,  0.02551832],\n",
      "       [-0.03356836, -0.01920515,  0.17762487, -0.21570033, -0.20671599,\n",
      "        -0.0206657 ,  0.28510514,  0.07240313,  0.03903392,  0.28568605],\n",
      "       [ 0.08036236, -0.12926273, -0.07896419, -0.26326486,  0.08760902,\n",
      "        -0.02954825,  0.21618395, -0.34462887,  0.04989397,  0.02163156],\n",
      "       [ 0.20965163,  0.24417798, -0.13194506, -0.11582881, -0.06312945,\n",
      "         0.08064634,  0.06686732,  0.12875734, -0.02787605,  0.10230352],\n",
      "       [ 0.00310194,  0.14960673,  0.22575778, -0.06317823,  0.14584178,\n",
      "        -0.23456088, -0.27551723, -0.05431201,  0.09221296, -0.16654366],\n",
      "       [-0.02308666, -0.09310532,  0.19292462, -0.31509519, -0.04029784,\n",
      "         0.10739518, -0.33111653, -0.32708061,  0.28234774,  0.25169864],\n",
      "       [ 0.04402636,  0.0637261 , -0.09118793, -0.14783093,  0.1316251 ,\n",
      "        -0.38484588, -0.2017688 , -0.31182945, -0.05847531,  0.25347418],\n",
      "       [ 0.09600824, -0.19112794,  0.12793076,  0.17178753,  0.07914569,\n",
      "        -0.29352054, -0.23978446,  0.25753734,  0.04197017, -0.02609897],\n",
      "       [ 0.12602498,  0.04760368,  0.10528725, -0.21033685, -0.17129946,\n",
      "         0.15792601,  0.00342613, -0.06761165, -0.18584105,  0.34132656],\n",
      "       [-0.00226979,  0.12062524, -0.13373338,  0.21589518, -0.04571426,\n",
      "        -0.14999081, -0.18237513, -0.52176988,  0.00542729, -0.03893818],\n",
      "       [-0.04529447,  0.03722511,  0.02771115, -0.08246803,  0.00235164,\n",
      "         0.10551245,  0.35713863,  0.09942088, -0.04840796,  0.01684736],\n",
      "       [-0.17393309,  0.15277772,  0.08322909,  0.07757173,  0.52389002,\n",
      "        -0.13016926, -0.08415087, -0.20670848,  0.15676436,  0.14740618],\n",
      "       [ 0.04264365,  0.04812581,  0.08458804, -0.22166808, -0.18605451,\n",
      "        -0.00174611,  0.39271745,  0.26738703, -0.10724096, -0.10128193],\n",
      "       [-0.07973607, -0.1116038 ,  0.01930647, -0.12069971,  0.18078119,\n",
      "        -0.31935406, -0.01222523, -0.13758084,  0.19505274,  0.26742721],\n",
      "       [ 0.03798064,  0.05514735,  0.21771276,  0.06840314, -0.08998801,\n",
      "        -0.12193158,  0.09273377,  0.25434461,  0.01369359,  0.12697914],\n",
      "       [ 0.11109123,  0.0321398 ,  0.21113314, -0.21406648,  0.08740035,\n",
      "         0.11778616,  0.40234995,  0.28425133, -0.1937055 , -0.05115689],\n",
      "       [-0.00745515, -0.17062436,  0.102773  ,  0.28417078,  0.10671833,\n",
      "        -0.33382568, -0.10168635, -0.77849126,  0.18145396, -0.15805219],\n",
      "       [-0.28992975,  0.15257905, -0.1450617 ,  0.03834378,  0.00530835,\n",
      "        -0.17649099, -0.08386953,  0.06217396,  0.02694388, -0.28820473],\n",
      "       [-0.03643746, -0.20702583, -0.05749732, -0.20017238, -0.02927344,\n",
      "         0.16191325,  0.25077221, -0.40060097, -0.12750162,  0.06204376],\n",
      "       [ 0.30580941,  0.08498025, -0.02956789, -0.20239677, -0.04830471,\n",
      "        -0.05453657,  0.08816816, -0.35804361,  0.02793228,  0.11317985],\n",
      "       [-0.08405515, -0.24262261, -0.16373308,  0.0550052 ,  0.06172956,\n",
      "         0.04524829,  0.13288431, -0.2002807 ,  0.07159306,  0.28263083],\n",
      "       [-0.10043278,  0.02298021, -0.03817962,  0.00894434,  0.2254073 ,\n",
      "         0.01753052,  0.07179449,  0.1568262 ,  0.04225303,  0.0543768 ],\n",
      "       [ 0.05909776,  0.01539287, -0.03485425, -0.10596266,  0.18973389,\n",
      "        -0.10707252, -0.00365214, -0.50784945, -0.05088238,  0.1601343 ],\n",
      "       [-0.09883121,  0.11070239,  0.05727227,  0.06777808,  0.24755831,\n",
      "         0.03468985,  0.25271261, -0.52758002,  0.17940222,  0.28700712],\n",
      "       [-0.1095647 ,  0.00754883,  0.05014785,  0.05875086,  0.01004459,\n",
      "        -0.09380987, -0.13717163,  0.17055376,  0.08339125, -0.22843401],\n",
      "       [-0.50046659,  0.16020501, -0.13196835,  0.251261  , -0.07062682,\n",
      "        -0.09120031, -0.12314732,  0.08343261, -0.01768332, -0.06510249],\n",
      "       [-0.10239077,  0.38201579,  0.04380824,  0.03477928,  0.11238983,\n",
      "        -0.14063346, -0.27856717, -0.21314804,  0.06470971, -0.16191649],\n",
      "       [-0.05611385, -0.11010139, -0.15169215, -0.00780084, -0.04638869,\n",
      "         0.2109592 ,  0.01741282, -0.50653154,  0.00376594,  0.22319293],\n",
      "       [-0.13876405, -0.07430362,  0.09149206, -0.23040882,  0.16705781,\n",
      "        -0.01436853,  0.07168497, -0.43914109,  0.16665906, -0.11976747]], dtype=float32), 'b3': array([ 0.        ,  0.0527554 ,  0.05464065,  0.        ,  0.05773973,\n",
      "        0.        ,  0.04953513, -0.38420725,  0.05158723,  0.05814774], dtype=float32), 'W2': array([[  4.84762117e-02,   1.85627654e-01,  -3.28230485e-03,\n",
      "         -1.97004199e-01,   6.38080668e-03,  -2.82131970e-01,\n",
      "         -3.65232117e-04,  -1.43931761e-01,   1.55783389e-02,\n",
      "          1.00024201e-01,   6.65464625e-02,   1.67410523e-01,\n",
      "         -3.01775225e-02,  -1.26788216e-02,   2.52849385e-02,\n",
      "          9.33177844e-02,   1.35550663e-01,  -2.52019733e-01,\n",
      "         -1.50500730e-01,  -9.78462547e-02,  -3.08707297e-01,\n",
      "          3.05214357e-02,  -6.35352060e-02,  -1.26981795e-01,\n",
      "         -1.28323557e-02,   2.92604059e-01,   1.01012498e-01,\n",
      "          4.56118375e-01,  -2.83902250e-02,   1.91996936e-02],\n",
      "       [ -2.25839213e-01,  -5.61854541e-02,  -6.17593899e-03,\n",
      "          1.63995102e-01,   1.34081304e-01,  -1.06366955e-01,\n",
      "         -1.13884332e-02,   2.47014444e-02,  -9.61105973e-02,\n",
      "          3.09398491e-02,  -1.27881840e-01,  -1.38063040e-02,\n",
      "         -1.45203128e-01,  -2.53258854e-01,   2.37475440e-01,\n",
      "          3.09001923e-01,  -5.38858697e-02,  -1.13450466e-02,\n",
      "         -9.66063142e-02,  -9.69060138e-02,   3.68602835e-02,\n",
      "          4.27541435e-01,   9.84635130e-02,  -9.36196148e-02,\n",
      "         -1.89602509e-01,   3.32085718e-03,  -1.26182005e-01,\n",
      "          5.61317384e-01,   2.74819076e-01,  -2.38036141e-01],\n",
      "       [ -8.56761113e-02,  -8.28954056e-02,   2.32127473e-01,\n",
      "         -1.35953706e-02,  -1.40832793e-02,   1.56733215e-01,\n",
      "          1.23030528e-01,   1.53644115e-01,  -6.01506047e-02,\n",
      "         -9.11109746e-02,  -2.25210786e-01,   1.19873576e-01,\n",
      "          5.77269010e-02,   2.08198465e-02,   2.67461479e-01,\n",
      "          1.54023930e-01,   3.11969779e-02,  -6.55370876e-02,\n",
      "         -2.24138439e-01,   7.07325935e-02,  -3.51693243e-01,\n",
      "         -2.03648731e-01,   1.77213512e-02,  -2.06091747e-01,\n",
      "         -1.70277268e-01,  -4.90490496e-02,   1.37784099e-03,\n",
      "         -5.00003219e-01,   2.48614788e-01,   2.85416543e-01],\n",
      "       [  1.14817908e-02,   5.17973602e-02,   1.83265045e-01,\n",
      "          2.92064786e-01,   1.52734205e-01,  -1.40160546e-01,\n",
      "          3.73088680e-02,  -7.55755007e-02,   5.89900650e-02,\n",
      "          2.03843452e-02,   2.59142399e-01,   1.13882467e-01,\n",
      "         -1.61839753e-01,  -3.22895259e-01,   1.16835549e-01,\n",
      "          8.61545876e-02,   2.17408121e-01,  -7.92271644e-02,\n",
      "          2.50975937e-02,   1.40222665e-02,  -4.19293344e-02,\n",
      "          5.25057875e-02,  -9.78235602e-02,   1.56774893e-01,\n",
      "          1.29433572e-01,  -4.46539484e-02,   8.14778581e-02,\n",
      "         -7.15393841e-01,   8.19506869e-02,  -1.93090718e-02],\n",
      "       [  2.35270634e-02,   1.19834661e-01,  -1.30081668e-01,\n",
      "         -1.44554630e-01,  -3.25604491e-02,   5.31701632e-02,\n",
      "          4.63048881e-03,   1.39416814e-01,  -3.32420990e-02,\n",
      "         -1.22590186e-02,  -2.26689279e-01,   2.15224296e-01,\n",
      "         -7.86374882e-02,   8.26242566e-02,   7.28247762e-02,\n",
      "         -6.10112771e-02,   7.08567873e-02,  -1.43687278e-01,\n",
      "          2.13264704e-01,  -1.51290894e-01,   4.66435701e-02,\n",
      "         -1.14324100e-01,   6.05037203e-03,   4.76411022e-02,\n",
      "          6.71746507e-02,   2.36096650e-01,   2.29552224e-01,\n",
      "          6.15032315e-01,   1.02011207e-02,   1.72318071e-02],\n",
      "       [ -7.76028959e-03,  -1.52428851e-01,   1.34767324e-01,\n",
      "          1.73210889e-01,   8.29360038e-02,  -2.56722152e-01,\n",
      "         -9.38037857e-02,  -7.51077160e-02,   3.73926610e-01,\n",
      "         -1.58127248e-01,   1.20583810e-01,   6.40670583e-02,\n",
      "         -9.98721868e-02,  -5.00961244e-02,  -2.93539744e-02,\n",
      "         -1.55044302e-01,  -3.13711213e-03,   7.49857947e-02,\n",
      "          3.56230140e-01,  -1.90884918e-01,   2.06408948e-01,\n",
      "          1.48109406e-01,  -2.32669428e-01,   8.80316719e-02,\n",
      "          1.34574741e-01,  -2.29773641e-01,  -2.26605073e-01,\n",
      "         -6.39898360e-01,  -7.84302428e-02,  -4.23708931e-02],\n",
      "       [ -1.82416514e-01,   2.46618539e-02,  -6.75088465e-02,\n",
      "          3.17732364e-01,  -4.64622118e-02,   1.60659790e-01,\n",
      "          1.19260602e-01,  -1.79060530e-02,   1.31986290e-01,\n",
      "          1.77512281e-02,  -9.25523520e-04,   3.64689231e-02,\n",
      "          2.36969724e-01,   1.47821292e-01,   6.45428449e-02,\n",
      "         -1.92855876e-02,  -8.47246721e-02,   3.80045064e-02,\n",
      "         -2.28575766e-01,   1.13338783e-01,  -1.81003630e-01,\n",
      "          4.08088490e-02,   5.02850302e-02,  -1.22205675e-01,\n",
      "          5.11995181e-02,  -1.75908953e-01,   6.22334657e-03,\n",
      "          6.85135245e-01,  -1.46750137e-01,   1.33692712e-01],\n",
      "       [ -3.53327274e-01,  -2.24316847e-02,  -6.96125999e-02,\n",
      "          1.45352900e-01,   2.14429498e-02,  -2.15952367e-01,\n",
      "         -1.02373838e-01,   1.72224686e-01,  -5.70266359e-02,\n",
      "         -7.43639395e-02,   8.05292279e-02,   6.35715201e-02,\n",
      "          1.63228884e-02,   1.26029685e-01,  -4.55283910e-01,\n",
      "          6.23774575e-03,  -1.98447462e-02,   1.44143999e-01,\n",
      "         -3.57149899e-01,  -1.07884288e-01,   1.22837752e-01,\n",
      "          2.23377421e-01,   3.32495943e-02,   1.48221835e-01,\n",
      "         -8.87601674e-02,  -1.28340483e-01,   1.28786415e-01,\n",
      "         -6.16526365e-01,   1.99850947e-01,   2.23099198e-02],\n",
      "       [  1.82648614e-01,   4.29731756e-02,  -8.52911845e-02,\n",
      "         -8.25511590e-02,   1.23857819e-02,   1.08080842e-01,\n",
      "          2.01225877e-01,   1.01191059e-01,   6.42436743e-02,\n",
      "          1.69202536e-01,   1.52037635e-01,  -1.32485658e-01,\n",
      "          6.28912598e-02,  -6.58814013e-02,  -1.43508255e-01,\n",
      "         -3.77016403e-02,   4.00555879e-02,   4.33198251e-02,\n",
      "          7.18314259e-04,   2.52600253e-01,  -1.78859517e-01,\n",
      "          5.37418306e-01,   5.47697730e-02,   1.38052613e-01,\n",
      "          1.34250522e-01,  -1.46522224e-02,  -5.24902120e-02,\n",
      "         -3.35576266e-01,   1.07773796e-01,   2.62681022e-02],\n",
      "       [  7.08027557e-02,  -5.25325947e-02,   1.67787626e-01,\n",
      "          9.65096429e-02,   2.59331703e-01,  -1.04368135e-01,\n",
      "         -9.67094004e-02,  -4.58562896e-02,   2.83516139e-01,\n",
      "         -1.04959682e-01,   8.61069337e-02,  -1.06271811e-01,\n",
      "         -2.20318705e-01,  -1.80951834e-01,  -1.54921561e-01,\n",
      "         -3.38388979e-03,   4.59179193e-01,  -6.96930066e-02,\n",
      "          7.99486637e-02,  -6.15838394e-02,  -7.60303363e-02,\n",
      "         -1.20047636e-01,   2.47002825e-01,  -1.64075688e-01,\n",
      "         -1.35787770e-01,  -1.50534540e-01,  -7.39903450e-02,\n",
      "          4.54826295e-01,   6.01480156e-03,  -1.64458826e-01],\n",
      "       [  1.00691237e-01,  -4.79090810e-02,   2.80798227e-01,\n",
      "          2.09265664e-01,  -9.81886126e-03,   2.11244643e-01,\n",
      "         -2.15925500e-02,  -1.62485495e-01,   7.74556175e-02,\n",
      "          1.54067978e-01,  -1.45502642e-01,  -1.17183283e-01,\n",
      "          9.24867913e-02,  -3.16953361e-02,   2.43467659e-01,\n",
      "         -2.74342626e-01,  -6.85519800e-02,   2.35696763e-01,\n",
      "          4.35849614e-02,   1.02228271e-02,   2.40860805e-01,\n",
      "         -1.10863326e-02,   2.72988565e-02,  -1.65339977e-01,\n",
      "          9.74903926e-02,   1.07535474e-01,  -1.99009061e-01,\n",
      "          7.79265165e-01,  -2.69885659e-02,  -5.93639277e-02],\n",
      "       [  2.77987514e-02,   1.26477137e-01,  -2.26306796e-01,\n",
      "         -9.22603831e-02,   1.22089021e-01,  -6.82629347e-02,\n",
      "         -2.80362546e-01,  -4.62366007e-02,   2.24989116e-01,\n",
      "          6.93978695e-03,   4.84593585e-02,   1.34703189e-01,\n",
      "         -8.29456672e-02,   7.10759833e-02,  -8.68322402e-02,\n",
      "         -6.50613829e-02,   3.40444222e-02,  -7.74526671e-02,\n",
      "         -2.24649787e-01,   2.07260028e-01,  -1.85250804e-01,\n",
      "          7.77077079e-02,   2.18006641e-01,  -2.54642248e-01,\n",
      "          2.16668636e-01,   8.76273662e-02,   3.50469768e-01,\n",
      "          5.03732860e-01,  -3.56207113e-03,   5.88047020e-02],\n",
      "       [  1.06993042e-01,   1.62739009e-01,   1.45417213e-01,\n",
      "         -1.32277727e-01,   4.43582892e-01,   9.70749930e-02,\n",
      "          1.02570221e-01,  -6.55951947e-02,   2.15344667e-01,\n",
      "          9.53118578e-02,   1.05927540e-02,   1.95860237e-01,\n",
      "         -1.83290228e-01,   3.06948334e-01,  -1.72229726e-02,\n",
      "          1.95497245e-01,   9.59123820e-02,   1.25747606e-01,\n",
      "         -8.09697658e-02,   4.39227894e-02,   4.07645069e-02,\n",
      "         -3.42372507e-01,  -1.70175210e-01,   9.47591383e-03,\n",
      "         -1.55201063e-01,   3.02519590e-01,   2.51568928e-02,\n",
      "          1.38397485e-01,   1.91590503e-01,   2.28521302e-02],\n",
      "       [ -2.12512195e-01,   4.76851910e-02,  -3.45235914e-02,\n",
      "         -6.23896383e-02,  -2.53969699e-01,  -4.50196952e-01,\n",
      "          2.15989966e-02,  -4.15188186e-02,   2.72143837e-02,\n",
      "         -1.44779235e-01,  -5.68897650e-02,  -3.71478945e-01,\n",
      "          2.72339433e-01,  -7.69833550e-02,   7.07786679e-02,\n",
      "         -1.63953751e-01,   2.79097021e-01,   8.25931802e-02,\n",
      "         -1.22766696e-01,   1.69147909e-01,   1.60454512e-02,\n",
      "          9.75864604e-02,   1.32582948e-01,   8.90634954e-02,\n",
      "         -5.87124471e-03,   5.60189068e-01,  -3.18680108e-02,\n",
      "         -6.30963266e-01,   7.41357133e-02,  -3.50157507e-02],\n",
      "       [  1.72215521e-01,  -2.62828648e-01,   1.09112576e-01,\n",
      "          3.04627717e-01,   4.31917198e-02,   1.64207742e-01,\n",
      "          1.87803775e-01,  -1.25867888e-01,  -4.33004946e-02,\n",
      "          3.77146393e-01,   5.85127063e-02,   2.03143045e-01,\n",
      "          4.29081470e-02,  -1.44050255e-01,   2.83376515e-01,\n",
      "         -2.87729740e-01,   1.43053710e-01,  -1.58287078e-01,\n",
      "         -1.04048327e-01,  -1.04644328e-01,  -1.34669468e-02,\n",
      "          1.55820757e-01,   1.16940185e-01,  -1.30521152e-02,\n",
      "         -6.23234399e-02,  -1.02362990e-01,  -3.93464901e-02,\n",
      "         -7.99103260e-01,   8.79120827e-03,  -1.99943528e-01],\n",
      "       [  6.77709877e-02,  -4.70794477e-02,   9.71424729e-02,\n",
      "         -2.15878084e-01,   5.07701188e-02,  -2.19747439e-01,\n",
      "         -1.06563464e-01,   3.15145016e-01,  -1.73455819e-01,\n",
      "         -2.68388927e-01,  -3.57924938e-01,  -1.32693738e-01,\n",
      "          1.81682147e-02,  -5.75876385e-02,  -6.89771399e-03,\n",
      "          1.18659258e-01,   9.21937004e-02,   1.83609221e-02,\n",
      "          3.85520756e-02,   2.63042748e-02,   9.76570789e-03,\n",
      "          2.37287097e-02,  -9.09463167e-02,  -1.90730527e-01,\n",
      "          1.67510465e-01,   3.05703953e-02,  -3.79872210e-02,\n",
      "          4.05193806e-01,   4.49498137e-03,  -1.29604712e-02],\n",
      "       [  5.12324721e-02,  -1.34734493e-02,   1.32149845e-01,\n",
      "          3.74067463e-02,  -6.48203939e-02,  -6.48661926e-02,\n",
      "          2.02255771e-02,   3.24540697e-02,  -1.37000337e-01,\n",
      "          1.11591436e-01,   4.00558896e-02,  -1.44984853e-02,\n",
      "         -7.59154707e-02,   5.14841788e-02,   2.26730049e-01,\n",
      "          1.52053654e-01,   5.69594502e-02,  -1.29812002e-01,\n",
      "         -1.81552067e-01,   1.38763681e-01,   7.49538466e-03,\n",
      "          1.00994602e-01,   2.32732117e-01,   1.19548365e-01,\n",
      "          1.31543562e-01,  -1.18388481e-01,   8.94747004e-02,\n",
      "          2.66620994e-01,   3.37382257e-02,  -1.64934725e-01],\n",
      "       [ -3.08329165e-01,  -1.90559328e-01,   7.10979402e-02,\n",
      "          1.45599633e-01,  -2.94171453e-01,   2.23284721e-01,\n",
      "          2.86043227e-01,   2.71143138e-01,  -2.52100021e-01,\n",
      "         -6.04667217e-02,  -1.44393951e-01,   1.92655146e-01,\n",
      "          3.59764285e-02,   1.24846406e-01,  -1.10229924e-01,\n",
      "          4.89918441e-02,   2.11910963e-01,   2.37291470e-01,\n",
      "          1.68948159e-01,   1.49188593e-01,   3.45220417e-01,\n",
      "         -1.38558835e-01,  -3.84088121e-02,  -1.13184907e-01,\n",
      "          4.36604112e-01,  -6.32314831e-02,   1.24677019e-02,\n",
      "         -5.83984971e-01,   2.11487144e-01,  -2.54430711e-01],\n",
      "       [  1.25154659e-01,   7.69286379e-02,   1.68335810e-01,\n",
      "         -1.29679680e-01,  -1.03168882e-01,   2.40400106e-01,\n",
      "         -2.45958976e-02,  -2.32716590e-01,   1.66644961e-01,\n",
      "         -1.25479043e-01,   3.40993345e-01,   1.03924602e-01,\n",
      "         -2.01511923e-02,  -1.67456642e-01,   2.25529373e-02,\n",
      "         -1.42875180e-01,   1.68611899e-01,  -1.68453619e-01,\n",
      "          1.90900356e-01,  -1.55453131e-01,   3.51199098e-02,\n",
      "         -1.81543395e-01,  -5.79597056e-02,   1.70439333e-01,\n",
      "         -2.47310668e-01,  -2.73425996e-01,  -3.10468495e-01,\n",
      "          5.48915625e-01,  -2.42756363e-02,  -2.74482787e-01],\n",
      "       [  1.74519584e-01,   5.26537970e-02,   1.68376535e-01,\n",
      "         -3.60352814e-01,  -2.90532440e-01,   4.17952128e-02,\n",
      "         -2.74285793e-01,   1.81113593e-02,   4.41034995e-02,\n",
      "          5.05331531e-02,   3.70290950e-02,   4.62922901e-02,\n",
      "         -7.79578835e-02,  -1.00967146e-01,  -2.26921186e-01,\n",
      "          7.76833892e-02,   2.24994883e-01,   3.35077569e-02,\n",
      "          3.44301075e-01,  -9.48474556e-02,  -1.24088563e-01,\n",
      "         -6.38455823e-02,   8.15840960e-02,  -1.12529650e-01,\n",
      "          2.23285958e-01,   7.24842325e-02,  -2.62038615e-02,\n",
      "          3.67204368e-01,  -6.13584369e-02,  -3.55361104e-01]], dtype=float32), 'b2': array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         7.45058060e-09,   0.00000000e+00,   3.72529030e-09,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         1.19209290e-07,   0.00000000e+00,   0.00000000e+00], dtype=float32), 'gamma2': array([ -2.08623614e-03,   5.58225764e-03,   4.90297750e-03,\n",
      "        -1.38398418e-02,   1.54931308e-03,  -2.57317605e-03,\n",
      "        -1.05809085e-02,  -1.50927762e-02,  -3.70931998e-03,\n",
      "        -2.09250906e-03,   6.83043711e-03,   4.88572335e-03,\n",
      "        -2.11247988e-02,   9.64165293e-03,  -3.29065546e-02,\n",
      "         7.17237545e-03,   1.20549416e-02,   4.64070663e-02,\n",
      "        -3.78969708e-03,  -1.01808188e-02,  -1.30763184e-02,\n",
      "        -2.94314232e-02,   5.94665980e-05,   1.19181359e-02,\n",
      "         2.20107548e-02,  -1.70438574e-03,  -3.64394602e-03,\n",
      "        -2.25263573e-02,   9.47656482e-03,   2.33982646e-04], dtype=float32), 'beta2': array([ -2.08665803e-03,   5.58317546e-03,   4.95834602e-03,\n",
      "        -1.38413496e-02,   1.54992077e-03,  -2.57693045e-03,\n",
      "        -1.05822831e-02,  -1.51020978e-02,  -3.71131790e-03,\n",
      "        -2.14551203e-03,   6.83649909e-03,   4.90137842e-03,\n",
      "        -2.11314615e-02,   9.75348707e-03,  -3.33916843e-02,\n",
      "         7.17507955e-03,   1.21292956e-02,   4.64270301e-02,\n",
      "        -3.80541012e-03,  -1.01950802e-02,  -1.30844861e-02,\n",
      "        -2.94372719e-02,   5.94996382e-05,   1.19198579e-02,\n",
      "         2.20759343e-02,  -1.70584454e-03,  -3.66868195e-03,\n",
      "        -2.61089206e-02,   9.48132575e-03,   2.34054402e-04], dtype=float32), 'W1': array([[ -4.55151722e-02,  -2.63218850e-01,   2.35148489e-01,\n",
      "          2.62610819e-02,  -2.83513200e-02,  -1.61441267e-01,\n",
      "         -4.91893571e-03,  -5.53615913e-02,  -1.41740322e-01,\n",
      "          1.43678132e-02,  -2.10376024e-01,   2.02924326e-01,\n",
      "         -4.70767245e-02,  -3.40975374e-01,   8.29663649e-02,\n",
      "          1.91315174e-01,   2.19894886e-01,   2.90610224e-01,\n",
      "          4.95519750e-02,   4.42788452e-02],\n",
      "       [  1.44717157e-01,  -7.42626488e-02,   1.86705992e-01,\n",
      "         -1.46635562e-01,  -1.57849610e-01,  -2.13324338e-01,\n",
      "          2.17064828e-01,   2.14419723e-01,   7.86230788e-02,\n",
      "         -5.40931463e-01,   2.36536581e-02,  -2.03239173e-01,\n",
      "          2.44153365e-02,  -2.62363404e-01,   2.80998111e-01,\n",
      "          1.26787415e-02,   2.97888089e-02,   4.14446741e-01,\n",
      "          2.49083668e-01,  -1.66534752e-01],\n",
      "       [  6.07698038e-02,   6.39145598e-02,  -1.21196225e-01,\n",
      "         -4.61184442e-01,   3.50164063e-02,  -1.00343935e-02,\n",
      "          1.78126559e-01,   2.82567181e-02,  -9.92055461e-02,\n",
      "          1.10381186e-01,   3.07692200e-01,   7.60150030e-02,\n",
      "         -3.82848352e-01,  -2.30776101e-01,  -1.91597506e-01,\n",
      "          7.11734071e-02,   1.56514749e-01,   3.26045379e-02,\n",
      "          4.16411340e-01,   3.36718202e-01],\n",
      "       [ -5.19972667e-02,   1.76584303e-01,  -2.81122848e-02,\n",
      "          1.85999185e-01,  -1.49417575e-02,   6.89924583e-02,\n",
      "          1.22239664e-01,  -2.60759026e-01,  -1.29450619e-01,\n",
      "         -2.27705806e-01,  -2.13640854e-02,   4.73974682e-02,\n",
      "         -1.75218582e-01,  -6.00398444e-02,   2.93772575e-02,\n",
      "         -5.74415401e-02,   2.40078777e-01,  -1.44160599e-01,\n",
      "         -7.11150616e-02,  -2.78961480e-01],\n",
      "       [  2.10504353e-01,   8.71724188e-02,   4.92084846e-02,\n",
      "         -1.04911499e-01,  -2.33565658e-01,  -1.39319316e-01,\n",
      "          6.62490651e-02,   7.21818879e-02,  -9.82984081e-02,\n",
      "          3.08534950e-01,   2.21685343e-03,   2.38024354e-01,\n",
      "          2.35987306e-02,   9.61928740e-02,  -3.63209401e-03,\n",
      "         -7.18722790e-02,  -2.22477376e-01,   2.21148163e-01,\n",
      "         -5.30357324e-02,  -1.51927724e-01],\n",
      "       [  1.32308051e-01,   2.87517548e-01,   1.67675011e-04,\n",
      "         -7.97804594e-02,   4.16091047e-02,  -5.81930988e-02,\n",
      "         -3.06780785e-02,   7.19639212e-02,   1.67629376e-01,\n",
      "          6.87393546e-02,  -1.21551111e-01,   2.11984366e-01,\n",
      "          8.59130695e-02,  -4.20633852e-02,   4.32173461e-02,\n",
      "          1.77088119e-02,  -4.23682556e-02,  -4.03576083e-02,\n",
      "         -1.15086893e-02,  -1.48926646e-01],\n",
      "       [ -2.08905786e-02,   4.04187232e-01,   7.87490606e-02,\n",
      "         -1.38404965e-01,  -1.44273862e-01,   7.09052831e-02,\n",
      "          2.71035209e-02,  -3.70374650e-01,   8.49121287e-02,\n",
      "          6.12931466e-03,   9.83552337e-02,  -2.73875952e-01,\n",
      "         -9.28346738e-02,  -2.27128416e-02,  -1.77418545e-01,\n",
      "          1.52189478e-01,  -1.76544368e-01,  -2.63487577e-01,\n",
      "         -2.03014851e-01,  -3.01067442e-01],\n",
      "       [ -2.73101106e-02,  -6.59243688e-02,   1.07512698e-01,\n",
      "         -1.40289783e-01,  -2.69103438e-01,  -1.83108672e-02,\n",
      "         -1.94287479e-01,  -1.85742695e-02,  -2.84209937e-01,\n",
      "          2.16009561e-02,   4.86553162e-02,   8.14953297e-02,\n",
      "          6.70755878e-02,   1.48444518e-01,  -2.36726478e-01,\n",
      "         -8.52523595e-02,  -4.77423072e-02,   2.42766023e-01,\n",
      "         -1.24609463e-01,  -1.53374702e-01],\n",
      "       [ -5.24055697e-02,  -1.76476970e-01,  -1.37904197e-01,\n",
      "          8.68890956e-02,  -1.37119249e-01,  -2.13328049e-01,\n",
      "         -1.57269523e-01,  -5.22012375e-02,  -6.70326650e-02,\n",
      "          6.56627715e-02,   1.82896227e-01,   1.47683784e-01,\n",
      "         -2.11694479e-01,   1.95532799e-01,  -2.39198729e-02,\n",
      "          1.35261342e-01,  -3.50085250e-03,   2.33094960e-01,\n",
      "         -5.35395145e-02,   1.12858199e-01],\n",
      "       [ -6.22241013e-03,   2.55046844e-01,   1.61093384e-01,\n",
      "          1.32440776e-01,  -5.92290936e-03,  -1.15111195e-01,\n",
      "          4.88949120e-02,   1.26558647e-01,  -2.55574673e-01,\n",
      "          2.13580817e-01,   6.26184046e-02,  -2.36993022e-02,\n",
      "          7.82502070e-03,   1.83160286e-02,  -2.91402668e-01,\n",
      "          1.93131894e-01,   1.56429857e-01,  -7.22002313e-02,\n",
      "          2.39542555e-02,  -3.85515988e-02],\n",
      "       [  1.96852729e-01,  -6.61033243e-02,   2.79022790e-02,\n",
      "          3.82778086e-02,   8.15664679e-02,   1.08627632e-01,\n",
      "         -4.92959097e-02,   2.32765883e-01,  -5.13768755e-02,\n",
      "          6.88244328e-02,   1.03503980e-01,  -2.92568445e-01,\n",
      "          7.28745013e-03,   1.88026890e-01,   5.30514792e-02,\n",
      "         -3.61108966e-02,  -2.65401691e-01,  -1.81681290e-02,\n",
      "          1.09501578e-01,   1.94337904e-01],\n",
      "       [  2.17229817e-02,   3.11241113e-02,  -1.28995046e-01,\n",
      "         -6.90593570e-03,  -6.04843535e-02,  -4.54470545e-01,\n",
      "         -1.15810618e-01,  -1.43345565e-01,   6.81955889e-02,\n",
      "         -4.08096798e-02,   3.84195119e-01,  -1.20266125e-01,\n",
      "         -5.69164455e-02,   3.46039720e-02,   3.83687437e-01,\n",
      "          2.00574789e-02,  -8.02422464e-02,   1.18231036e-01,\n",
      "          1.47921905e-01,  -1.35288969e-01],\n",
      "       [  2.86623925e-01,   6.05467744e-02,   2.11958990e-01,\n",
      "          5.81484810e-02,  -1.24558762e-01,  -3.16890627e-02,\n",
      "          1.14252500e-01,   1.12720340e-01,  -5.02391644e-02,\n",
      "          1.47179067e-01,   1.77277699e-01,  -1.26880556e-01,\n",
      "          3.03525329e-02,   1.61311075e-01,   1.32017920e-03,\n",
      "         -3.88803244e-01,   1.03028780e-02,   2.01495901e-01,\n",
      "         -1.39113851e-02,  -5.13434112e-02],\n",
      "       [ -2.06907824e-01,   1.11937046e-01,  -1.73134759e-01,\n",
      "          1.50185362e-01,  -5.10601662e-02,  -7.70531595e-02,\n",
      "          3.56234349e-02,   2.98408896e-01,  -9.77505594e-02,\n",
      "          5.11465333e-02,   1.53471842e-01,   1.67061597e-01,\n",
      "          9.18233097e-02,  -1.53256580e-01,   1.34609044e-01,\n",
      "         -1.28085318e-03,  -7.02071562e-02,   1.27736911e-01,\n",
      "         -1.82592958e-01,  -3.36455591e-02],\n",
      "       [  1.39605761e-01,   2.71796025e-02,   1.50557816e-01,\n",
      "         -3.33338022e-01,  -2.98986137e-01,  -7.02528236e-03,\n",
      "          4.39642780e-02,   7.61906281e-02,   5.85978180e-02,\n",
      "         -7.92263448e-02,   9.25442576e-02,   5.34136146e-02,\n",
      "          3.06585766e-02,   2.86942124e-01,   6.68205544e-02,\n",
      "         -1.09204017e-01,  -2.04831325e-02,   1.91001520e-01,\n",
      "          4.75265794e-02,   1.07509186e-02]], dtype=float32), 'b1': array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "        -1.19209290e-07,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,  -3.72529030e-09,\n",
      "        -9.31322575e-10,   0.00000000e+00], dtype=float32), 'gamma1': array([-0.01246365,  0.00612385, -0.00859312,  0.03016151,  0.0142986 ,\n",
      "        0.01820038,  0.0264104 ,  0.01736103, -0.03198604, -0.01447244,\n",
      "        0.04346506, -0.00617855, -0.06356677,  0.01576369,  0.04296999,\n",
      "       -0.02174203, -0.0435669 ,  0.00898329,  0.00293682, -0.03039327], dtype=float32), 'beta1': array([-0.01255022,  0.00612641, -0.00859514,  0.03022489,  0.01431153,\n",
      "        0.01826506,  0.02650291,  0.01736579, -0.03205994, -0.01447352,\n",
      "        0.04347882, -0.00617967, -0.06491346,  0.0158175 ,  0.04297798,\n",
      "       -0.02174914, -0.04358847,  0.00898671,  0.00293829, -0.0303997 ], dtype=float32)}\n",
      "Initial loss:  7.00323648274\n",
      "W1 relative error: 1.00e+00\n",
      "W2 relative error: 1.00e+00\n",
      "W3 relative error: 1.00e+00\n",
      "b1 relative error: 1.00e+00\n",
      "b2 relative error: 1.00e+00\n",
      "b3 relative error: 1.00e+00\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "beta1 relative error: 1.00e+00\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "beta2 relative error: 1.00e+00\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.]\n",
      "gamma1 relative error: 1.00e+00\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "gamma2 relative error: 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float32,\n",
    "                            use_batchnorm=True)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    if len(name) > 2:\n",
    "        print(model.params[name])\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-6)\n",
    "    if name == 'beta2':\n",
    "        print(grads['beta2'], grad_num)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.float64(1.)\n",
    "a.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batchnorm for deep networks\n",
    "Run the following to train a six-layer network on a subset of 1000 training examples both with and without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 200) loss: 2.640032\n",
      "(Epoch 0 / 10) train acc: 0.085000; val_acc: 0.095000\n",
      "(Epoch 1 / 10) train acc: 0.231000; val_acc: 0.178000\n",
      "(Epoch 2 / 10) train acc: 0.423000; val_acc: 0.239000\n",
      "(Epoch 3 / 10) train acc: 0.465000; val_acc: 0.266000\n",
      "(Epoch 4 / 10) train acc: 0.560000; val_acc: 0.293000\n",
      "(Epoch 5 / 10) train acc: 0.592000; val_acc: 0.282000\n",
      "(Epoch 6 / 10) train acc: 0.672000; val_acc: 0.310000\n",
      "(Epoch 7 / 10) train acc: 0.681000; val_acc: 0.303000\n",
      "(Epoch 8 / 10) train acc: 0.730000; val_acc: 0.323000\n",
      "(Epoch 9 / 10) train acc: 0.768000; val_acc: 0.327000\n",
      "(Epoch 10 / 10) train acc: 0.783000; val_acc: 0.312000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gamma1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cf03b836cb27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 },\n\u001b[1;32m     32\u001b[0m                 verbose=True, print_every=200)\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/vlad/Documents/cs231n/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vlad/Documents/cs231n/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vlad/Documents/cs231n/assignment2/cs231n/classifiers/fc_net.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mfirst_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_bn_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gamma1'"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "bn_solver.train()\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the following to visualize the results from two networks trained above. You should find that using batch normalization helps the network to converge much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o', label='baseline')\n",
    "plt.plot(bn_solver.loss_history, 'o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.train_acc_history, '-o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(solver.val_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.val_acc_history, '-o', label='batchnorm')\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch normalization and initialization\n",
    "We will now run a small experiment to study the interaction of batch normalization and weight initialization.\n",
    "\n",
    "The first cell will train 8-layer networks both with and without batch normalization using different scales for weight initialization. The second layer will plot training accuracy, validation set accuracy, and training loss as a function of the weight initialization scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers = {}\n",
    "solvers = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print('Running weight scale %d / %d' % (i + 1, len(weight_scales)))\n",
    "  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers[weight_scale] = bn_solver\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot results of weight scale experiment\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(1.0, 3.5)\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question:\n",
    "Describe the results of this experiment, and try to give a reason why the experiment gave the results that it did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Answer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
